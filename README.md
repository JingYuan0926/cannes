# SEInsightğŸ‘‡
**An AI Agent Data Analyst build in TEE(Trusted Execution Environment)**
![MainPage](https://github.com/JingYuan0926/cannes/blob/main/public/MainPage.svg?raw=true)


An AI-powered data analysis platform that processes data within a Trusted Execution Environment (TEE), providing intelligent insights through conversational AI while maintaining complete data privacy and confidentiality. No more data leaks, no more privacy concerns, just secure, powerful analysis that keeps your data safe.

We've deployed a live demonstration of **(SEInsight)** at [here](https://f81f-83-144-23-155.ngrok-free.app/).

---

## Inspiration: How We Came Up with This Idea ğŸ’¡

We noticed two critical problems in the data analysis landscape:

**For Small and Medium Enterprises (SMEs):**
Hiring a data analyst is prohibitively expensive. The average salary for a data analyst ranges from $60,000 to $120,000 annually, plus benefits and training costs. For SMEs operating on tight budgets, this is simply unaffordable, leaving them without access to crucial business insights that could drive growth and efficiency.

**For Fortune 500 Enterprises and Governments:**
Data leakage is a constant threat. High-profile breachesâ€”from government agencies to Fortune 500 companiesâ€”make headlines weekly. Even with expensive security measures, sensitive data often ends up in the wrong hands, causing financial losses, reputational damage, and regulatory penalties.

We thought:

> *"What if we could create an AI Agent data analyst that's both affordable for SMEs and secure enough for enterprises and governments?"*

This exploration led us to design a privacy-first AI data analyst that operates within a Trusted Execution Environment (TEE), providing enterprise-grade analysis capabilities at a fraction of the cost while ensuring complete data security and confidentiality.

---

## The Problem ğŸš§

Organizations and individuals face several significant challenges when attempting to analyze documents and extract insights:

**1. Privacy Concerns:** Sending sensitive documents to third-party AI services poses unacceptable risks for many organizations, especially when dealing with confidential business data or personal information.

**2. Technical Barriers:** Traditional data analysis tools require technical expertise, making them inaccessible to non-technical users who need insights from their documents.

**3. Fragmented Workflows:** Users often need to juggle multiple toolsâ€”document uploaders, analysis platforms, and reporting toolsâ€”creating inefficient and error-prone processes.

**4. Limited Context:** Most AI analysis tools provide one-off responses without maintaining conversation context, making it difficult to ask follow-up questions or explore deeper insights.

**5. Cost Prohibitive:** Enterprise-grade AI analysis tools often come with high costs, making them inaccessible to smaller organizations and individuals.

---

## The Solution ğŸ”‘

Our AI Data Analyst Chatbot addresses these challenges by implementing a secure, conversational document analysis platform:

**1. Privacy-First Architecture:** All document processing and AI analysis occurs within Trusted Execution Environments (TEEs), ensuring that sensitive data aren't accessible to anyone, not even users themselves.

**2. Adaptive Insight Cards:** Dynamically displays relevant insight cards based on user needs and data content, hiding irrelevant information to provide focused, actionable intelligence. Users can view both individual insight cards and a comprehensive summary of all insights.

**3. Three-Tier Analysis:** Provides descriptive (what happened), predictive (what will happen), and prescriptive (what should be done) analysis capabilities for comprehensive data insights.

**4. Conversational Interface:** Maintains context throughout the session, allowing users to ask follow-up questions and explore insights naturally.

**5. Enterprise-Grade AI:** Powered by OpenAI, providing sophisticated analysis capabilities while maintaining the security and privacy of sensitive documents.

**6. Source Citations:** Includes reputable sources with clickable hyperlinks, providing transparency and credibility to AI-generated insights.

In short, our platform enables users to upload encrypted data, ask questions in natural language and receive intelligent, contextual responsesâ€”all while maintaining complete data privacy and security.

---

## How Our Project Works âš™ï¸

### Project Flow 1: Document Upload & Analysis

1. **Upload Encrypted Data**
   - Users upload encrypted data through our secure interface
   - Data is automatically processed within the secure environment
   - Encrypted content is decrypted and prepared for AI analysis

2. **Establish Context**
   - The AI system processes the uploaded encrypted data to understand the content
   - A session ID is generated to maintain conversation context
   - Users receive confirmation that their data is ready for analysis

3. **Conversational Analysis**
   - Users ask questions about the data in natural language
   - The AI provides concise, actionable insights (2-4 sentences by default)
   - Follow-up questions maintain context from previous interactions
   - Users can view adaptive insight cards that display relevant information based on their needs
   - A summary of all insight cards is provided for comprehensive overview

4. **Three Types of Analysis**
   - **Descriptive Analysis**: Understand what happened in the past with detailed data summaries and trends
   - **Predictive Analysis**: Forecast future outcomes and trends based on historical patterns
   - **Prescriptive Analysis**: Provide actionable recommendations and next steps for optimal decision-making

5. **Detailed Analysis (Optional)**
   - Users can request detailed explanations by explicitly asking for more information
   - The AI provides comprehensive analysis when requested
   - Source citations are included with clickable hyperlinks

---

## System Architecture High-Level Overview ğŸ—ï¸

![Architecture](https://github.com/JingYuan0926/cannes/blob/main/public/Architecture.svg?raw=true)

The current architecture consists of a Next.js frontend, OpenAI API integration, and secure document processing pipeline. This setup provides two clear user pathways:

![Architecture](https://github.com/JingYuan0926/cannes/blob/main/public/Architecture.svg?raw=true)

![UserFlow](https://github.com/JingYuan0926/cannes/blob/main/public/UserFlow.svg?raw=true)



### Document Analysis Flow 

- **File Upload**: Users upload PDF or DOCX files through the secure interface
- **Document Processing**: Files are stored using Walrus
- **Session Management**: UUID-based session tracking maintains conversation context
- **AI Analysis**: GPT-4o Mini processes document content and user queries
- **Response Generation**: Formatted responses with source citations and insight cards are returned to users

### Direct Chat Flow (Without File Upload - Green Line)

- **User Input**: Users ask questions directly without uploading encrypted data
- **Context Management**: Conversation history is maintained for coherent responses
- **AI Processing**: GPT-40 Mini analyzes queries and provides intelligent insights
- **Response Delivery**: Formatted responses with relevant source citations and insight cards

This dual-flow system architecture ensures flexibility, security, and user-friendly interaction while maintaining data privacy and providing enterprise-grade analysis capabilities.

---

## Tech Stack Overview ğŸ› ï¸

- **Next.js 15.3.5** â€“ Front-end React framework with API routes
- **OpenAI API** â€“ GPT-4 integration for intelligent analysis
- **ReactMarkdown** â€“ Rich text rendering with HTML support
- **Multer** â€“ File upload handling and processing
- **PDF-Parse** â€“ PDF document text extraction
- **Mammoth** â€“ DOCX document processing
- **UUID** â€“ Session management and tracking
- **Tailwind CSS** â€“ UI styling and responsive design
- **Rehype-Raw** â€“ HTML rendering for source citations
- **Remark Plugins** â€“ Markdown formatting and processing

---

## Important Code Directories ğŸ“‚

Here's a brief overview of important directories in our repository:

- **`/pages`**
  - **`chat.js`**: Main chat interface with file upload and conversation management
  - **`api/chatbot.js`**: AI API endpoint handling both file uploads and chat conversations
  - **`_app.js`**: App wrapper and global configuration

- **`/styles`**
  - **`globals.css`**: Global styles including ReactMarkdown formatting

- **`/public`**
  - Static assets and icons for the application

- **`/package.json`**
  - Project dependencies including OpenAI, document processing libraries, and UI components

---

## How We Are Different ğŸŒŸ

Unlike traditional document analysis tools, our platform focuses on **conversational AI and privacy-first architecture** from the **user's perspective**. We're not just processing documents; we're creating an intelligent, secure, and accessible analysis experience. Here's how our **AI Agent Data Analyst** stands out:

| **Feature**                          | **Traditional  Analysis Bot**                                     | **AI Agent Data Analyst**                                                                 |
|--------------------------------------|-----------------------------------------------------------------------|------------------------------------------------------------------------------------------|
| **Privacy & Security**               | Often requires uploading to third-party servers with limited privacy guarantees | **TEE-based architecture** with secure encrypted data processing and automatic cleanup |
| **Conversational Interface**         | Typically one-off analysis with no follow-up capability | **Maintains conversation context** allowing natural follow-up questions and deeper exploration |
| **Encrypted Data Processing**        | Often limited to specific file formats | **Seamless encrypted data processing** with complete security |
| **Source Citations**                 | Rarely includes source attribution | **Reputable sources with clickable hyperlinks** for transparency and credibility |
| **Response Formatting**              | Plain text or basic formatting | **Rich HTML formatting** with proper styling and structure |
| **Accessibility**                    | Often requires technical expertise | **User-friendly interface** accessible to non-technical users |
| **Cost Effectiveness**               | Expensive enterprise solutions | **Affordable AI analysis** powered by OpenAI's GPT-4 |

**AI Data Analyst Chatbot** goes beyond traditional document analysis by offering a **secure, conversational, and accessible solution** that prioritizes privacy while providing enterprise-grade analysis capabilities.

---

## Future Implementations ğŸš€

We are committed to continuously improving our platform and expanding its capabilities. Here are some exciting future plans:

### Stage 1
- **Better Machine Learning Model Performance**
- **Mass Support of Machine Learning Algorithm**
- **GPU TEE for Better LLM Performance**

- ### Before vs After Report System ğŸ”
- **Impact Tracking**: Monitor the effectiveness of AI recommendations over time
- **Visual Comparisons**: Generate before/after charts and metrics showing measurable improvements
- **ROI Measurement**: Demonstrate concrete value and return on investment from following AI advice
- **Decision Optimization**: Help users understand which recommendations yield the best results
- **Progress Visualization**: Show cumulative improvements and trends in decision-making effectiveness
  
### Stage 2
- **Access Control for Different Management Level**
- **Go Live on MainNet**

---

## Team ğŸ‘¥

- **Derek Liew Qi Jian**  
  - *Role*: Project Lead, AI & TEE Integration
  - [LinkedIn](https://www.linkedin.com/in/derek2403/) | [Twitter](https://x.com/derek2403)

- **Phen Jing Yuan**  
  - *Role*: Project Lead, AI & Backend Integration
  - [LinkedIn](https://www.linkedin.com/in/jing-yuan-phen-b42266295/) | [Twitter](https://x.com/ilovedahmo)

- **Yeoh Choon Xiang**  
  - *Role*: Frontend Developer & UI/UX Design 
  - [LinkedIn](https://www.linkedin.com/in/choon-xiang-yeoh/) | [Twitter](https://x.com/choonxiangg)

- **Choong Zhao Zheng**  
  - *Role*: Frontend Developer & UI/UX Design
  - [LinkedIn](https://www.linkedin.com/in/choong-zhao-zheng-business-information-systems/)

- **Yee Lee Ren (Johnny)**  
  - *Role*: Frontend Developer & UI/UX Design
  - [LinkedIn](https://www.linkedin.com/in/ee-mun-leong-700a23316/) | [Twitter](https://x.com/14jren)












# ğŸš€ AI Data Analysis Pipeline

A comprehensive AI-powered data analysis system with microservices architecture, featuring ETL processing, data preprocessing, exploratory data analysis (EDA), and machine learning analysis with TEE (Trusted Execution Environment) attestation.

## ğŸ—ï¸ Architecture

The pipeline consists of 4 microservices:

- **ETL Service** (Port 3030) - Data extraction, transformation, and loading
- **Preprocessing Service** (Port 3031) - Data cleaning and preparation
- **EDA Service** (Port 3035) - Exploratory data analysis and visualization
- **Analysis Service** (Port 3040) - Machine learning analysis with TEE attestation

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop installed and running
- OpenAI API key

### Setup

1. **Clone and navigate to the project:**
   ```bash
   git clone <your-repo-url>
   cd cannes
   ```

2. **Create environment file:**
   ```bash
   cp .env.example .env
   ```

3. **Edit `.env` file with your OpenAI API key:**
   ```bash
   # OpenAI Configuration
   OPENAI_API_KEY=your_actual_openai_api_key_here
   OPENAI_MODEL=gpt-4o-mini
   OPENAI_TEMPERATURE=0.7
   ```

4. **Start all services:**
   ```bash
   ./start-services.sh
   ```

   Or manually:
   ```bash
   docker-compose up -d
   ```

## ğŸ“Š Services Overview

### ETL Service (Port 3030)
- **Endpoint:** `http://localhost:3030`
- **Purpose:** File upload and initial data processing
- **Supported formats:** CSV, Excel, JSON

### Preprocessing Service (Port 3031)
- **Endpoint:** `http://localhost:3031`
- **Purpose:** Data cleaning, normalization, and ML preparation
- **Features:** Missing value handling, feature engineering

### EDA Service (Port 3035)
- **Endpoint:** `http://localhost:3035`
- **Purpose:** Exploratory data analysis and visualization
- **Features:** Statistical analysis, interactive charts, insights generation

### Analysis Service (Port 3040)
- **Endpoint:** `http://localhost:3040`
- **Purpose:** Machine learning analysis with TEE attestation
- **Features:** ML algorithms, model evaluation, cryptographic attestation

## ğŸ”§ Management Commands

```bash
# View service logs
docker-compose logs -f

# Stop all services
docker-compose down

# Restart services
docker-compose restart

# Check service status
docker-compose ps

# Pull latest images
docker-compose pull
```

## ğŸŒ Web Interface

The pipeline works with your Next.js web application. Make sure to:

1. Start the Docker services (as described above)
2. Run your Next.js application:
   ```bash
   npm run dev
   # or
   yarn dev
   ```
3. Navigate to your analyze page to upload and analyze data

## ğŸ“ Directory Structure

```
cannes/
â”œâ”€â”€ docker-compose.yml          # Master compose file
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ .env                       # Your actual environment variables
â”œâ”€â”€ start-services.sh          # Service startup script
â”œâ”€â”€ data/                      # Data storage directory
â”œâ”€â”€ results/                   # Analysis results directory
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ analyze.js            # Analysis web interface
â”‚   â””â”€â”€ upload.js             # File upload interface
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ app.py                # Analysis service with TEE
â”‚   â”œâ”€â”€ tee_signing.py        # TEE attestation module
â”‚   â””â”€â”€ compose.yaml          # Individual service compose
â”œâ”€â”€ eda/
â”‚   â””â”€â”€ compose.yaml          # EDA service compose
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ compose.yaml          # ETL service compose
â””â”€â”€ preprocessing/
    â””â”€â”€ compose.yaml          # Preprocessing service compose
```

## ğŸ” TEE Attestation

The Analysis Service includes Trusted Execution Environment (TEE) attestation:

- **Integrity Protection:** Cryptographic hashing ensures data hasn't been tampered with
- **Authenticity:** Digital signatures prove the analysis origin
- **Non-repudiation:** ROFL app ID provides cryptographic proof of execution

## ğŸ› Troubleshooting

### Services Not Starting
```bash
# Check Docker is running
docker info

# Check service logs
docker-compose logs [service-name]

# Restart specific service
docker-compose restart [service-name]
```

### Port Conflicts
If ports are already in use, modify the port mappings in `docker-compose.yml`:
```yaml
ports:
  - "3031:3030"  # Change left port number
```

### Environment Variables
Ensure your `.env` file has the correct OpenAI API key:
```bash
# Check current environment
docker-compose config
```

## ğŸ“ˆ Usage Flow

1. **Upload Data** â†’ ETL Service processes the file
2. **Preprocessing** â†’ Data is cleaned and prepared
3. **EDA Analysis** â†’ Generates visualizations and insights
4. **ML Analysis** â†’ Runs machine learning algorithms with TEE attestation
5. **Results** â†’ View comprehensive analysis with cryptographic verification

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with the local Docker setup
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Happy Analyzing! ğŸ‰**
